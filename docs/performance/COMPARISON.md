# Renderer Performance Comparison

> Generated by `tools/bench/compare_mermaid_renderers.py`.

## Environment

- Timestamp: "2026-02-10 18:00:09 +0800"
- OS: "Windows-11-10.0.26200-SP0"
- Machine: "AMD64"
- CPU: "13th Gen Intel(R) Core(TM) i9-13900KF"
- Python: "3.13.11"
- Node: "v24.13.0"
- Chromium: "HeadlessChrome/131.0.6778.204"
- mermaid-cli: "11.12.0"
- merman: `5853894c74c6eeb16aad14b8018f57a06b5575aa`
- mermaid-rs-renderer: `859253415e69dce28bd65cd5a7c1d1ae8b39f4a1`
- mermaid-js: `mermaid@11.12.2`
- Rust:

```
rustc 1.93.0 (254b59607 2026-01-19)
binary: rustc
commit-hash: 254b59607d4417e9dffbc307138ae5c86280fe4c
commit-date: 2026-01-19
host: x86_64-pc-windows-msvc
release: 1.93.0
LLVM version: 21.1.8
```

## Method

- `merman`: `cargo bench -p merman --features render --bench pipeline -- ...`
- `mermaid-rs-renderer` (mmdr): `cargo bench --bench renderer -- ...`
- Filter: "end_to_end/(flowchart_tiny|flowchart_medium|flowchart_large|sequence_tiny|sequence_medium|state_tiny|state_medium|class_tiny|class_medium)"
- Sample size: 20, warm-up: 1s, measurement: 1s

## Results (end_to_end, mid estimate)

| benchmark | merman | mermaid-rs-renderer | mermaid-js (puppeteer) | ratio (merman / mmdr) | ratio (merman / mermaid-js) |
|---|---:|---:|---:|---:|---:|
| end_to_end/class_medium | 23.34 ms | 2.52 ms | 98.40 ms | 9.3x | 0.2x |
| end_to_end/class_tiny | 777.61 µs | 33.57 µs | 4.30 ms | 23.2x | 0.2x |
| end_to_end/flowchart_medium | 34.60 ms | 4.62 ms | 72.70 ms | 7.5x | 0.5x |
| end_to_end/flowchart_tiny | 684.84 µs | 33.54 µs | 5.00 ms | 20.4x | 0.1x |
| end_to_end/sequence_medium | 315.55 µs | 345.05 µs | 6.30 ms | 0.9x | 0.05x |
| end_to_end/sequence_tiny | 89.13 µs | 28.08 µs | 3.00 ms | 3.2x | 0.03x |
| end_to_end/state_medium | 8.13 ms | 1.97 ms | 40.40 ms | 4.1x | 0.2x |
| end_to_end/state_tiny | 866.34 µs | 34.95 µs | 6.20 ms | 24.8x | 0.1x |

## Notes

- `merman` is parity-focused (upstream Mermaid SVG DOM gates) and optimized for deterministic alignment.
- `mermaid-rs-renderer` is a different renderer with different goals and coverage; raw performance numbers are not directly comparable to visual/DOM parity.
- Treat these as **local regression tracking** numbers. Always re-run on the same machine/toolchain for meaningful comparisons.

## Skipped (merman)

These fixtures were present but skipped because `merman` returned a parse/layout/render error during the pre-check.

`treemap_medium`, `xychart_medium`
